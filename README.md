### 武将データの可視化ツールの実行方法
### 要件
* Ubuntu 16.04
* Anaconda python 3.6（Scikit-learn, numpy, matplotlibなどの標準的なパッケージ込み）

### 実行例
以下のようにanalyze_warrior.pyを実行すると、T-SNEで次元削減しクラスタリングされた武将データの２次元プロットと、各クラスターに関連する語、および各クラスターに属する武将名の一覧を表示することができます。。

python analyze_warrior.py
 
![alt text](https://raw.githubusercontent.com/hhachiya/invereFunction/master/analysis_cluster.png "clustering result")

```python
Enterを押すと, 各クラスター中心に関連する語top5を表示します
クラスター0 (-9.1,4.8): ['家臣', '宗直', '頼房', '義直', '城主']
クラスター1 (2.3,3.1): ['主君', '旧主', '老臣', '家康', '自刃']
クラスター2 (10.2,-3.7): ['名苗', '眞廣', '本阿弥', '饗する', '佐定']
クラスター3 (-1.6,-2.2): ['忠吉', '信綱', '秀宗', '高房', '義直']
クラスター4 (-2.7,4.5): ['家康', '秀吉', '家久', '主君', '清正']
クラスター5 (6.0,-1.0): ['近習', '眞廣', '沖室', '徳川家', '国元']
クラスター6 (11.3,1.9): ['鑑', '本阿弥', '宗長', '家伝', '眞廣']
クラスター7 (-6.0,-0.6): ['彦四郎', '信実', '頼房', '忠重', '老臣']
クラスター8 (2.6,-3.9): ['家康', '老臣', '家久', '主君', '一豊']
クラスター9 (6.5,-5.7): ['家来', '命', '方', '饗する', '佐定']

Enterを押すと, クラスター0に属する武将一覧を表示します:
['朝比奈信置' '穴山信友' '井伊直満' '井伊直義' '石亀政頼' '板垣信憲' '海野昌雪' '岡部貞綱' '奥山朝忠' '奥山朝利'
 '小河信章' '長船定行' '小田氏治' '小野朝直' '小野朝之' '小野政直' '小山田行村' '上泉秀綱_(主水佐)'
 '小寺則職_(戦国時代)' '斎藤利堯' '関盛信' '高梨内記' '竹田津鎮満' '土岐定政' '中野直由' '鍋島忠茂' '林秀貞'
 '北条氏照' '堀田興重' '松田直長' '水野忠分' '武藤安成' '室賀久太夫']
 
 Enterを押すと, クラスター1に属する武将一覧を表示します:
['朝倉宗滴' '尼子国久' '臼杵鎮続' '飯富虎昌' '柿崎景家' '柏山明助' '金森長近' '兼重元宣' '河田長親' '北条高広'
 '香宗我部親泰' '佐伯惟治' '真田幸隆' '陶興房' '陶晴賢' '高橋紹運' '団忠正' '富田長繁' '内藤弘矩' '丹羽長秀'
 '橋本一巴' '北条氏邦' '北条綱成' '三宅長盛' '三好長慶' '三好義興' '森可成' '吉岡長増']
 ...
 ```

### word2vecとT-SNEを用いたライブドアのニュース記事の可視化

### 概要
ライブドアのニュース記事ごとに、ドキュメント特徴量を抽出します。
ドキュメント特徴量の抽出方法：
1. 記事に出現する単語のword2vec特徴量の平均


### 要件
* Ubuntu 16.04
* python 3.6

### インストール方法

#### Mecab
以下を参照して、インストールします。
http://hirotaka-hachiya.hatenablog.com/entry/2017/10/05/130026

#### word2vec
1. pip install gensim

#### livedoor-news-data
ライブドアのニュース記事データのtgzファイルをダウンロードし、livedoor-news-dataに置き、解凍します。
ライブドアのニュース記事は、カテゴリごとにxmlで記載されています。
1. cd livedoor-news-data
2. wget https://www.rondhuit.com/download/livedoor-news-data.tar.gz
3. tar -xvzf livedoor-news-data.tar.gz
4. cd ..

### 使い方
上記のインストールが完了した状態で、以下のようにword2vec_livedoor.pyを実行します。 

python word2vec_livedoor.py tfidf T F T

word2vec_livedoor.pyの引数は、
* 抽出方法(方式1:'w2v',方式2:'tfidf')
* 記事データへの処理を行うフラグ
* 辞書データの作成・更新を行うフラグ
* word2vecモデルの学習を行うフラグ
となっています。

### 各ニュースカテゴリに対する処理（方式１の場合）の説明
1. livedoor-news-dataからxmlの記事データを読み込む。
2. titleとbodyアイテムの値を抽出したテキストデータを、記事ごとに１行ずつlivedoor-news-data-txtに書き出す。
3. テキストデータにMecabで形態素解析し、形態素ごとに半角スペースで区切った形態素データをlivedoor-news-wakatiに書き出す。
4. 形態素データに対し、word2vecをかけて、記事ごとに平均を取ったベクトル（100次元）をlivedoor-news-pklに書き出す。つまり、pklには、記事の数 X 100次元の行列が保存される。

### 各ニュースカテゴリに対する処理（方式２の場合）の説明
1. livedoor-news-dataからxmlの記事データを読み込む。
2. titleとbodyアイテムの値を抽出したテキストデータを、記事ごとに１行ずつlivedoor-news-data-txtに書き出す。
3. テキストデータにMecabで形態素解析し、形態素ごとに半角スペースで区切った形態素データをlivedoor-news-wakatiに書き出す。
4. 形態素ごとに'形態素','全記事での出現回数','出現した文章の数','idf値'のデータフレームを作成、dataにcsv形式で書き出す。
5. 形態素データに対し、tf値を求める。同時にword2vecをかけて、形態素ごとのベクトル（100次元）にTF-IDF値をかけ合わせたものを記事ごとに平均したものをlivedoor-news-pklに書き出す。つまり、pklには、記事の数 X 100次元の行列が保存される。

